Friday, 10/27/2023 at 3 PM


Notes

One goal may be to maximize the height of the left most bars in an Enrichment Factor Plot.
Another goal may be to improve the correlation of predicted and observed docking scores.
Another goal may be to maximize the ratio of the range of predicted docking scores to the range of observed docking scores.

See below dictionary of models and maximum enrichment factors for feature matrix of docking scores and numbers of occurrences of substructures.
Bayesian model using a BART model: ?
Bayesian Linear Regression: ?
Linear Regression: ?
BART: ?
BNN: ?

See below dictionary of models and ratios of range of predicted docking score to range of observed docking score.
BNN: [? - ?] / (?- ?] = ?.
Linear Regression: [? - ?] / (?- ?] = ?.
Bayesian Linear Regression: [? - ?] / (?- ?] = ?.
Bayesian model using BART model: [? - ?] / (?- ?] = ?.

The below Decile-Wise Lift Chart was created based on Data Frame Of 1,060,613 Observed Docking Scores And Averages And Standard Deviations Of Docking Scores Predicted By BNN Based On Numbers Of Occurrences Of Substructures. A bin is a group of adjacent observed docking scores in the above data frame. A value of baseline or an enrichment factor are computed for each of 10 adjacent bins as

  (proportion of observed docking scores in bin in lowest 10 percent)/(proportion of observed docking scores in bin)

In computing a value of baseline, the rows in the above data frame are randomized. In computing an enrichment factor, the rows in the above data frame are sorted in ascending order by average predicted docking score.

Bayesian model using BART model results in the highest enrichment factor but the lowest range of predicted docking scores.
BNN results in the lowest enrichment factor but the highest range of predicted docking scores. BNN predicts docking scores less accurately than Bayesian model using BART model for mean-like observed docking scores, but predicts docking scores more accurately than Bayesian model using BART model for extreme observed docking scores.


Future Considerations

Improve performance of not Linear Regression but BART and Bayesian Linear Regression models, Bayesian model using BART model, and Bayesian Neural Network. Increase variance of priors for BNN.
Consider using different sets of descriptors and aggregated numbers of occurrences of substructures and Bayesian Model Averaging (https://www.kaggle.com/code/billbasener/bayesian-model-averaging-regression-tutorial).
Consider https://github.com/online-ml/river for training a Bayesian model on a data frame with 1,024 columns related to folded sums of number of occurrences of substructures.
Consider Model Averaging to compare models / types of models or to combine models in an ensemble (https://www.pymc.io/projects/docs/en/v3.11.4/pymc-examples/examples/diagnostics_and_criticism/model_averaging.html).
Consider developing a Random Forest model.
Consider developing an ensemble of a Random-Forest model and a Linear-Regression model.
Consider addressing warnings like the following warnings. Perhaps increase number of draws or reduce number of descriptors.
The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
Consider an ROC Curve.
Consider a Gains Curve.
Consider lazypredict -> LazyRegression.